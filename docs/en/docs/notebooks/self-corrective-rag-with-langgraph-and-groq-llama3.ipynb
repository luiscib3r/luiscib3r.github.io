{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='flex gap-1 items-center'>\n",
    "    <img alt='self llama' src='../images/self-rag.jpeg' width='128' height='128' class='rounded'>\n",
    "    <h1>Self Corrective RAG with LangGraph and Groq Llama 3</h1>\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-core langchain-community langchain-groq langgraph wikipedia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_community.retrievers import WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you! How can I assist you today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model='llama3-groq-8b-8192-tool-use-preview'\n",
    ")\n",
    "\n",
    "res = llm.invoke('Hi! How are you today?')\n",
    "\n",
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Meta AI',\n",
       " 'summary': \"Meta AI is an American company owned by Meta (formerly Facebook) that develops artificial intelligence and augmented and artificial reality technologies. Meta AI deems itself an academic research laboratory, focused on generating knowledge for the AI community, and should not be confused with Meta's Applied Machine Learning (AML) team, which focuses on the practical applications of its products.\",\n",
       " 'source': 'https://en.wikipedia.org/wiki/Meta_AI'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = WikipediaRetriever(top_k_results=6)\n",
    "\n",
    "docs = retriever.invoke(\"Meta AI\")\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta AI is an American company owned by Meta (formerly Facebook) that develops artificial intelligence and augmented and artificial reality technologies. Meta AI deems itself an academic research laboratory, focused on generating knowledge for the AI community, and should not be confused with Meta's Applied Machine Learning (AML) team, which focuses on the practical applications of its products. \n",
      "\n",
      "The laboratory was founded as Facebook Artificial Intelligence Research (FAIR) with locations at the headquarters in Menlo Park, California, London, United Kingdom, and a new laboratory in Manhattan. FAIR was officially announced in September 2013. FAIR was first directed by New York University's Yann LeCun, a deep learning professor and Turing Award winner. Working with NYU's Center for Data Science, FAIR's initial goal was to research data science, machine learning, and artificial intelligence and to \"understand intelligence, to discover its fundamental principles, and to make machines significantly more intelligent\". Research at FAIR pioneered the technology that led to face recognition, tagging in photographs, and personalized feed recommendation. Vladimir Vapnik, a pioneer in statistical learning, joined FAIR in 2014. Vapnik is the co-inventor of the support-vector machine and one of the developers of the Vapnik–Chervonenkis theory.\n",
      "FAIR opened a research center in Paris, France in 2015, and subsequently launched smaller satellite research labs in Seattle, Pittsburgh, Tel Aviv, Montreal and London. In 2016, FAIR partnered with Google, Amazon, IBM, and Microsoft in creating the Partnership on Artificial Intelligence to Benefit People and Society, an organization with a focus on open licensed research, supporting ethical and efficient research practices, and discussing fairness, inclusivity, and transparency.\n",
      "In 2018, Jérôme Pesenti, former CTO of IBM's big data group, assumed the role of president of FAIR, while LeCun stepped down to serve as chief AI scientist. In 2018, FAIR was placed 25th in the AI Research Rankings 2019, which ranked the top global organizations leading AI research. FAIR quickly rose to eighth position in 2019, and maintained eighth position in the 2020 rank. FAIR had approximately 200 staff in 2018, and had the goal to double that number by 2020.\n",
      "FAIR's initial work included research in learning-model enabled memory networks, self-supervised learning and generative adversarial networks, text classification and translation, as well as computer vision. FAIR released Torch deep-learning modules as well as PyTorch in 2017, an open-source machine learning framework, which was subsequently used in several deep learning technologies, such as Tesla's autopilot  and Uber's Pyro. Also in 2017, FAIR discontinued a research project once AI bots developed a language that was unintelligible to humans, inciting conversations about dystopian fear of artificial intelligence going out of control. However, FAIR clarified that the research had been shut down because they had accomplished their initial goal to understand how languages are generated, rather than out of fear.\n",
      "FAIR was renamed Meta AI following the rebranding that changed Facebook, Inc. to Meta Platforms Inc.\n",
      "In 2022, Meta AI predicted the 3D shape of 600 million potential proteins in two weeks.\n",
      "\n",
      "Artificial intelligence communication requires a machine to understand natural language and to generate language that is natural. Meta AI seeks to improve these technologies to improve safe communication regardless of what language the user might speak. Thus, a central task involves the generalization of natural language processing (NLP) technology to other languages. As such, Meta AI actively works on unsupervised machine translation. Meta AI seeks to improve natural-language interfaces by developing aspects of chitchat dialogue such as repetition, specificity, response-relatedness and question-asking, incorporating personality into image captioning, and generating creati\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    formatted = [\n",
    "        (\n",
    "            f\"Source ID: {i+1}\\n\"\n",
    "            f\"Article Title: {doc.metadata['title']}\\n\"\n",
    "            f\"Article URL: {doc.metadata['source']}\\n\"\n",
    "            f\"Article Content: {doc.page_content}\"\n",
    "        )\n",
    "        for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Source ID: 1\n",
      "Article Title: Meta AI\n",
      "Article URL: https://en.wikipedia.org/wiki/Meta_AI\n",
      "Article Content: Meta AI is an American company owned by Meta (formerly Facebook) that develops artificial intelligence and augmented and artificial reality technologies. Meta AI deems itself an academic research laboratory, focused on generating knowledge for the AI community, and should not be confused with Meta's Applied Machine Learning (AML) team, which focuses on the practical applications of its products. \n",
      "\n",
      "The laboratory was founded as Facebook Artificial Intelligence Research (FAIR) with locations at the headquarters in Menlo Park, California, London, United Kingdom, and a new laboratory in Manhattan. FAIR was officially announced in September 2013. FAIR was first directed by New York University's Yann LeCun, a deep learning professor and Turing Award winner. Working with NYU's Center for Data Science, FAIR's initial goal was to research data science, machine learning, and artificial intelligence and to \"understand intelligence, to discover its fundamental principles, and to make machines significantly more intelligent\". Research at FAIR pioneered the technology that led to face recognition, tagging in photographs, and personalized feed recommendation. Vladimir Vapnik, a pioneer in statistical learning, joined FAIR in 2014. Vapnik is the co-inventor of the support-vector machine and one of the developers of the Vapnik–Chervonenkis theory.\n",
      "FAIR opened a research center in Paris, France in 2015, and subsequently launched smaller satellite research labs in Seattle, Pittsburgh, Tel Aviv, Montreal and London. In 2016, FAIR partnered with Google, Amazon, IBM, and Microsoft in creating the Partnership on Artificial Intelligence to Benefit People and Society, an organization with a focus on open licensed research, supporting ethical and efficient research practices, and discussing fairness, inclusivity, and transparency.\n",
      "In 2018, Jérôme Pesenti, former CTO of IBM's big data group, assumed the role of president of FAIR, while LeCun stepped down to serve as chief AI scientist. In 2018, FAIR was placed 25th in the AI Research Rankings 2019, which ranked the top global organizations leading AI research. FAIR quickly rose to eighth position in 2019, and maintained eighth position in the 2020 rank. FAIR had approximately 200 staff in 2018, and had the goal to double that number by 2020.\n",
      "FAIR's initial work included research in learning-model enabled memory networks, self-supervised learning and generative adversarial networks, text classification and translation, as well as computer vision. FAIR released Torch deep-learning modules as well as PyTorch in 2017, an open-source machine learning framework, which was subsequently used in several deep learning technologies, such as Tesla's autopilot  and Uber's Pyro. Also in 2017, FAIR discontinued a research project once AI bots developed a language that was unintelligible to humans, inciting conversations about dystopian fear of artificial intelligence going out of control. However, FAIR clarified that the research had been shut down because they had accomplished their initial goal to understand how languages are generated, rather than out of fear.\n",
      "FAIR was renamed Meta AI following the rebranding that changed Facebook, Inc. to Meta Platforms Inc.\n",
      "In 2022, Meta AI predicted the 3D shape of 600 million potential proteins in two weeks.\n",
      "\n",
      "Artificial intelligence communication requires a machine to understand natural language and to generate language that is natural. Meta AI seeks to improve these technologies to improve safe communication regardless of what language the user might speak. Thus, a central task involves the generalization of natural language processing (NLP) technology to other languages. As such, Meta AI actively works on unsupervised machine translation. Meta AI seeks to improve natural-language interfaces by developing aspects of chitchat dialogue such as repetition, specificity, response-relatedness and question-asking, incorporating personality into image captioning, and generating creati\n",
      "\n",
      "Source ID: 2\n",
      "Article Title: Llama (language model)\n",
      "Article URL: https://en.wikipedia.org/wiki/Llama_(language_model)\n",
      "Article Content: Llama (acronym for Large Language Model Meta AI, and formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.1, released in July 2024.\n",
      "Model weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 7B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\n",
      "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n",
      "\n",
      "After the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\n",
      "Compared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\n",
      "\n",
      "LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
      "Llama was trained on only publicly available information, and was trained at various different model sizes, with the intention to make it more accessible to different hardware.\n",
      "Meta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n",
      "\n",
      "On March 3, 2023, a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities. That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation. On March 4, a pull request was opened to add links to HuggingFace repositories containing the model. On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests. On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.\n",
      "Reactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments. Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.\n",
      "\n",
      "On July 18, 2023, in partnership with Microsoft, Meta announ\n"
     ]
    }
   ],
   "source": [
    "print(format_docs(docs[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question with citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CitedAnswer Output Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources.\",\n",
    "    )\n",
    "    citations: list[int] = Field(\n",
    "        ...,\n",
    "        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI assistant. Your task is to answer questions based SOLELY on the information provided in the given articles.\n",
      "\n",
      "Main instructions:\n",
      "1. Answer the user's question using ONLY the information from the provided articles.\n",
      "2. Cite the source for EVERY statement you make, using the format [source_id] at the end of each sentence.\n",
      "3. If the information needed to answer the question is not in the provided articles, respond: \"I don't have enough information in the provided sources to answer this question.\"\n",
      "4. DO NOT use external knowledge or information not in the given articles, even if you believe it to be correct.\n",
      "5. If you can only partially answer the question, provide the information you have and then indicate that the rest of the question cannot be answered with the available information.\n",
      "6. Be concise and direct in your responses.\n",
      "\n",
      "Reference articles:\n",
      "\u001b[33;1m\u001b[1;3m{documents}\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RAG_SYSTEM_PROMPT = (\n",
    "    \"You are a helpful AI assistant. Your task is to answer questions based SOLELY on the information provided in the given articles.\\n\\n\"\n",
    "    \"Main instructions:\\n\"\n",
    "    \"1. Answer the user's question using ONLY the information from the provided articles.\\n\"\n",
    "    \"2. Cite the source for EVERY statement you make, using the format [source_id] at the end of each sentence.\\n\"\n",
    "    \"3. If the information needed to answer the question is not in the provided articles, respond: \\\"I don't have enough information in the provided sources to answer this question.\\\"\\n\"\n",
    "    \"4. DO NOT use external knowledge or information not in the given articles, even if you believe it to be correct.\\n\"\n",
    "    \"5. If you can only partially answer the question, provide the information you have and then indicate that the rest of the question cannot be answered with the available information.\\n\"\n",
    "    \"6. Be concise and direct in your responses.\\n\\n\"\n",
    "    \"Reference articles:\\n\"\n",
    "    \"{documents}\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", RAG_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "RAG_PROMPT.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_llm = llm.with_structured_output(CitedAnswer)\n",
    "\n",
    "rag_response_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        documents=(lambda x: format_docs(x[\"documents\"]))\n",
    "    )\n",
    "    | RAG_PROMPT\n",
    "    | rag_llm\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x['question']) | retriever\n",
    "\n",
    "rag_chain = RunnablePassthrough.assign(\n",
    "    documents=retrieve_docs\n",
    ").assign(response=rag_response_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CitedAnswer(answer='Meta AI is an American company owned by Meta (formerly Facebook) that develops artificial intelligence and augmented and artificial reality technologies.', citations=[1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke({\"question\": \"What is Meta AI?\"})\n",
    "\n",
    "result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CitedAnswer(answer='Jann LeCun is a French computer scientist and director of AI Research at Facebook. He is known for his work on convolutional neural networks and is a co-recipient of the Turing Award.', citations=[1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke({\"question\": \"Who is Jann LeCun?\"})\n",
    "\n",
    "result['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a query re-writer that converts an input query to an improved version optimized for Wikipedia search and retrieval. Your task is to generate ONE new query that is semantically related to the original and in the same domain. Do not provide explanations, ask questions, or add any text other than the new query. Analyze the input to understand its underlying intent, then output only the improved query.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "QUERY_REWRITER_SYSTEM = (\n",
    "    \"You are a query re-writer that converts an input query to an improved version optimized for Wikipedia search and retrieval. \"\n",
    "    \"Your task is to generate ONE new query that is semantically related to the original and in the same domain. \"\n",
    "    \"Do not provide explanations, ask questions, or add any text other than the new query. \"\n",
    "    \"Analyze the input to understand its underlying intent, then output only the improved query.\"\n",
    ")\n",
    "\n",
    "QUERY_REWRITER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", QUERY_REWRITER_SYSTEM),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{query}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "QUERY_REWRITER_PROMPT.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Rewriter Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meta AI technologies'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rewriter_chain = QUERY_REWRITER_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "result = query_rewriter_chain.invoke({\"query\": \"What is Meta AI?\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Context Relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckContextResponse Output Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckContextResponse(BaseModel):\n",
    "    \"\"\"Decide if the documents are relevant to the query.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Documents are relevants, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an evaluator for documents relevance. Given a set of documents and a query, you need to decide if the documents are relevant to the query. Your work is ask with `yes` if documents are relevant to the query or `no` if not. \n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "DOCUMENTS:\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{documents}\u001b[0m\n",
      "\n",
      "QUERY: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHECK_CONTEXT_SYSTEM = (\n",
    "    \"You are an evaluator for documents relevance. \"\n",
    "    \"Given a set of documents and a query, you need to decide if the documents are relevant to the query. \"\n",
    "    \"Your work is ask with `yes` if documents are relevant to the query or `no` if not. \"\n",
    ")\n",
    "\n",
    "CHECK_CONTEXT_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', CHECK_CONTEXT_SYSTEM),\n",
    "        ('user', \"DOCUMENTS:\\n\\n{documents}\\n\\nQUERY: {query}\\n\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "CHECK_CONTEXT_PROMPT.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Context Relevance Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_context_llm = llm.with_structured_output(CheckContextResponse)\n",
    "check_context_chain = CHECK_CONTEXT_PROMPT | check_context_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckContextResponse(binary_score='yes')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"Meta AI\")\n",
    "\n",
    "result = check_context_chain.invoke(\n",
    "    {\"documents\": format_docs(docs), \"query\": \"What is Meta AI?\"}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckContextResponse(binary_score='no')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = check_context_chain.invoke(\n",
    "    {\"documents\": format_docs(docs), \"query\": \"Who is Miyamoto Musashi?\"}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Hallucinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckHallucinations Output Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. Give a binary score 'yes' or 'no', where 'yes' means that the answer is grounded in / supported by the set of facts. IF the generation includes code examples, make sure those examples are FULLY present in the set of facts, otherwise always return score 'no'. \n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Set of facts: \n",
      "\n",
      " \u001b[33;1m\u001b[1;3m{documents}\u001b[0m \n",
      "\n",
      " LLM generation: \u001b[33;1m\u001b[1;3m{generation}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECK_HALLUCINATION_SYSTEM = (\n",
    "    \"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \"\n",
    "    \"Give a binary score 'yes' or 'no', where 'yes' means that the answer is grounded in / supported by the set of facts. \"\n",
    "    \"IF the generation includes code examples, make sure those examples are FULLY present in the set of facts, otherwise always return score 'no'. \"\n",
    ")\n",
    "\n",
    "CHECK_HALLUCINATION_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", CHECK_HALLUCINATION_SYSTEM),\n",
    "        (\"human\",\n",
    "         \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "CHECK_HALLUCINATION_PROMPT.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Hallucinations Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_hallucination_llm = llm.with_structured_output(CheckHallucinations)\n",
    "\n",
    "check_hallucination_chain = CHECK_HALLUCINATION_PROMPT | check_hallucination_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = 'Meta AI is an American company owned by Meta (formerly Facebook) that develops artificial intelligence and augmented and artificial reality technologies.'\n",
    "\n",
    "result = check_hallucination_chain.invoke(\n",
    "    {\"documents\": format_docs(docs), \"generation\": generation}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckHallucinations(binary_score='no')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = 'Miyamoto Musashi was legendary Japanese hero, artist and swordsman.'\n",
    "\n",
    "result = check_hallucination_chain.invoke(\n",
    "    {\"documents\": format_docs(docs), \"generation\": generation}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Answer Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckAnswer Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a grader assessing whether an answer addresses / resolves a query. Give a binary score 'yes' or 'no', where 'yes' means that the answer resolves the query. Your main language is spanish\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "User query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n",
      "LLM generation: \u001b[33;1m\u001b[1;3m{generation}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECK_ANSWER_SYSTEM = (\n",
    "    \"You are a grader assessing whether an answer addresses / resolves a query. \"\n",
    "    \"Give a binary score 'yes' or 'no', where 'yes' means that the answer resolves the query. \"\n",
    "    \"Your main language is spanish\"\n",
    ")\n",
    "\n",
    "CHECK_ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", CHECK_ANSWER_SYSTEM),\n",
    "        (\"human\",\n",
    "         \"User query: {query}\\n\\nLLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "CHECK_ANSWER_PROMPT.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_llm = llm.with_structured_output(CheckAnswer)\n",
    "\n",
    "check_answer_chain = CHECK_ANSWER_PROMPT | check_answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = 'Jann LeCun is a French computer scientist and director of AI Research at Facebook. He is known for his work on convolutional neural networks and is a co-recipient of the Turing Award.'\n",
    "\n",
    "result = check_answer_chain.invoke(\n",
    "    {\"query\": \"What is Jann LeCun?\", \"generation\": generation}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckAnswer(binary_score='no')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = check_answer_chain.invoke(\n",
    "    {\"query\": \"Who is Miyamoto Musashi?\", \"generation\": generation}\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luisciber-buq1kKJR-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
